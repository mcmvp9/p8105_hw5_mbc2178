---
title: "P8105_hw5_mbc2178"
author: "Melvin Coleman"
date: '`r format(Sys.time(), "%Y-%m-%d")`'
output: github_document
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(dplyr)
library(stringr)
set.seed(1)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

Let's define a function to calculate proportions with confidence intervals in 
this homework problem. This function will be used in problem 2.

```{r}
prop_fx = function(x,n) {
 
   prop.test(x,n) %>% 
    broom::tidy() %>% 
    
   select(estimate, conf.low, conf.high) %>% 
          mutate(estimate = round(estimate*100, 2),
                  conf.low = round(conf.low*100, 2),
                  conf.high = round(conf.high*100, 2))
}
```

### Problem 1 

```{r}

```


### Problem 2 

Let's examine the Washington Post's data on homicides in 50 large U.S. cities. 
We will refer to this raw dataset as `homicide_df`.

```{r}
homicide_df =
  read_csv('data/homicide-data.csv', col_names = TRUE) 
```

The `homicide_df` contains data from 50 large cities in the U.S. with `r ncol(homicide_df)`
fields/ variables and `r nrow(homicide_df)` records/observations. This dataset includes
first and last names as well as ethnicity of homicide victims (`victim_last`,`victim_first`,
`victim_race`), dates of homicide report(`reported_date`) and the status of cases characterized 
as closed with arrest or closed without arrest (`disposition`). In addition, the 
dataset also contains the longitude (`lon`) and latitude (`lat`) of the cities 
where the homicide occurred. 

Let's perform some cleaning and manipulation on our dataset. We first create 
a new variable `city_state` that combines the name of cities and states. 

```{r}
homicide_df =
  homicide_df %>% 
  janitor::clean_names() %>% 
  mutate(
    city_state = str_c(city,',',state)) 
```

We now summarize within cities to obtain the total number of homicides within each city
across the U.S. from our dataset. We created a new dataset called `tot_homicides`
containing just two variables; `city_state` containing the cities across states and 
`tot_homicides` containing the total number of homicides per city in the United 
States.

```{r}
tot_homicides =
  homicide_df %>% 
    group_by(city_state) %>% 
    summarise(tot_number_homicides = n()) %>% 
    arrange(desc(tot_number_homicides)) 
  
```
From our analysis, the city of Chicago has the largest number of total homicides 
in the United States with a total of 5,535 homicides. Tulsa,AL has the least 
number of homicides, just 1 homicide. This is most likely a data error as there 
is no city in the U.S in Alabama named Tulsa and logically doesn't make sense 
that this city would have just 1 homicide in total. 

We now summarize within cities to obtain the total number of unsolved homicides (those
for which the disposition was "Closed without arrest" or "Open/No arrest") across the
U.S. from our dataset.
We replaced all missing values with 0 for those cities that did not have any cases 
closed without arrest and/or open/no arrest. We used the `pivot wider` function 
to create a table that's easier to read with the columns being the disposition
status. We created a new variable called  `total_unsolved_homicides` that summed
the dispositions giving us the total unsolved homicides per city in the US.
We called this new dataset `unsolved_homicides`.

```{r}
unsolved_homicides =
  homicide_df %>% 
      group_by(city_state, disposition) %>% 
       filter(disposition %in% c('Closed without arrest', 'Open/No arrest')) %>% 
        summarize(n = n()) %>% 
     
    mutate(
      city_state = as.factor(city_state)) %>% 
        arrange(desc(n)) %>% 
          pivot_wider(
            names_from = disposition,
              values_from = n
  ) %>% 
      janitor::clean_names() %>% 
           replace(is.na(.),0) %>% 
  mutate(total_unsolved_homicides = sum(open_no_arrest, closed_without_arrest))
  
```
From our analysis, Chicago has the largest number of unsolved homicides in the US (4,073) with 
3,686 cases still open without any arrest and 387 cases closed without arrest. On
the other hand, Tampa has the least number of unsolved homicides in the US (95) with
87 cases still open without arrest and 8 cases closed without arrest.

We now merge the two datasets created above, `tot_homicides` and `unsolved_homicides`.
with 50 US cities (Tulsa, AL was ommitted from newly created dataset) and nested 
our homicide totals for each city. 
This will allow us to perform additional manipulations and answer specific questions 
using the variables created from the analysis conducted to create the datasets.
We kept the following variables in the newly created dataset, `homicide_mrg` :
`city_state`, `total_unsolved_homicides`, `tot_number_homicides`.  

```{r}
homicide_mrg =
  merge(unsolved_homicides, tot_homicides) %>% 
    select(city_state, total_unsolved_homicides, tot_number_homicides) %>% 
      arrange(desc(tot_number_homicides)) 
```

Now we examine the proportion of homicides that are unsolved for the city of Baltimore
by using the `prop.test` function.`homicide_prop`was created as a list that contains the
proportion estimates and confidence intervals for each city. We utilize the function
created above to run this procedure making use of `purrr::map` and `unnest` to 
produce a tidy dataframe. Output was saved as `prop_baltimore`.


```{r}
prop_baltimore =
  homicide_mrg %>% 
  filter(city_state %in% c('Baltimore,MD')) %>% 
  mutate( homicide_prop =
            map2(.x=total_unsolved_homicides, .y=tot_number_homicides, .f=prop_fx)) %>% 
  unnest(homicide_prop) %>% 
  select(estimate, conf.low, conf.high) 
 

prop_baltimore %>% 
   rename('Lower level CI'= conf.low ,
         'Upper Level CI' = conf.high,
         Proportion = estimate)
```
`r prop_baltimore %>% pull(estimate)` of homicides with a confidence interval of 
`r prop_baltimore %>% pull(conf.low)` and `r prop_baltimore %>% pull(conf.high)`
are unsolved in the city of Baltimore, MD. 

Now we run `prop.test` for each of the cities in our dataset, and extract both 
the proportion of unsolved homicides and the confidence interval for each city.
We apply the function created above making use of `purrr::map` and `unnest` to 
produce a tidy dataframe.`homicide_prop` variable was created as a list that 
contains the proportion estimates and confidence intervals for each city. 
Output was saved as `prop_cities`.

```{r}
prop_cities=
  homicide_mrg %>% 
      mutate(homicide_prop =
               map2(.x =total_unsolved_homicides,.y=tot_number_homicides,.f=prop_fx)) %>% 
    unnest(homicide_prop) %>% 
  select(city_state,estimate, conf.low, conf.high)

prop_cities %>% 
    rename('Lower level CI'= conf.low ,
         'Upper Level CI' = conf.high,
         Proportion = estimate,
         City = city_state)
```

Let's create a plot that shows the estimates and confidence intervals for each 
city adding error bars based on the upper and lower limits of the confidence interval. 
Cities are organized according to the proportion of unsolved homicides 

```{r}
prop_cities %>% 
  
  ggplot(aes(x= fct_reorder(city_state, -estimate), y= estimate)) + 
  geom_bar(stat="identity", color="black", fill="lightblue",
           position=position_dodge()) +
  scale_y_continuous(breaks = c(0,20,40,60,80,100)) +
  
  geom_errorbar(aes(ymin =conf.low, ymax= conf.high), width= .2,
                position=position_dodge(.9)) + 
  
  labs(title = "Proportion of unsolved homicides across 50 US cities") +
  xlab("Cities") +
  ylab("Proportions") +
  theme(axis.text.x = element_text(angle = 60, hjust = 1))
```

### Problem 3




