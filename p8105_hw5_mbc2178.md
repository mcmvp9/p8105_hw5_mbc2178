P8105_hw5_mbc2178
================
Melvin Coleman
2022-11-15

### Functions

Let’s define the function to calculate proportions with confidence
intervals in this homework problem. This function will be used in
problem 2.

``` r
  # Prop.test function
prop_fx = function(x,n) {
 
   prop.test(x,n) %>% 
    broom::tidy() %>% 
    
   select(estimate, conf.low, conf.high) %>% 
          mutate(estimate = round(estimate*100, 2),
                  conf.low = round(conf.low*100, 2),
                  conf.high = round(conf.high*100, 2))
}
```

Let’s define the function to perform t-test for a mean(s) and the
p-value arising from a test of H:μ=0 using α=0.05. Use `tidy.broom`
function to clean output, and round values to 3 digits. This function
will be used in problem 3.

``` r
  # T-test function 
  
  ttest_fx = function(true_mu){
    
     sample = rnorm(30, mean= true_mu, sd=5) 
    
    test_results = t.test(x=sample, mu=0) %>% 
      broom::tidy() %>% 
      select(estimate, p.value) %>% 
      
      # Round estimate & p-value to 3 digits
      mutate(
        estimate = round(estimate,3),
        p.value = round(p.value,3)
      )
  }
```

### Problem 1

### Problem 2

Let’s examine the Washington Post’s data on homicides in 50 large U.S.
cities. We will refer to this raw dataset as `homicide_df`.

``` r
homicide_df =
  read_csv('data/homicide-data.csv', col_names = TRUE) 
```

    ## Rows: 52179 Columns: 12
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (9): uid, victim_last, victim_first, victim_race, victim_age, victim_sex...
    ## dbl (3): reported_date, lat, lon
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

The `homicide_df` contains data from 50 large cities in the U.S. with 12
fields/ variables and 52179 records/observations. This dataset includes
first and last names as well as ethnicity of homicide victims
(`victim_last`,`victim_first`, `victim_race`), dates of homicide
report(`reported_date`) and the status of cases characterized as closed
with arrest or closed without arrest (`disposition`). In addition, the
dataset also contains the longitude (`lon`) and latitude (`lat`) of the
cities where the homicide occurred.

Let’s perform some cleaning and manipulation on our dataset. We first
create a new variable `city_state` that combines the name of cities and
states.

``` r
homicide_df =
  homicide_df %>% 
  janitor::clean_names() %>% 
  mutate(
    city_state = str_c(city,',',state)) 
```

We now summarize within cities to obtain the total number of homicides
within each city across the U.S. from our dataset. We created a new
dataset called `tot_homicides` containing just two variables;
`city_state` containing the cities across states and `tot_homicides`
containing the total number of homicides per city in the United States.

``` r
tot_homicides =
  homicide_df %>% 
    group_by(city_state) %>% 
    summarise(tot_number_homicides = n()) %>% 
    arrange(desc(tot_number_homicides)) 
```

From our analysis, the city of Chicago has the largest number of total
homicides in the United States with a total of 5,535 homicides. Tulsa,AL
has the least number of homicides, just 1 homicide. This is most likely
a data error as there is no city in the U.S in Alabama named Tulsa and
logically doesn’t make sense that this city would have just 1 homicide
in total.

We now summarize within cities to obtain the total number of unsolved
homicides (those for which the disposition was “Closed without arrest”
or “Open/No arrest”) across the U.S. from our dataset. We replaced all
missing values with 0 for those cities that did not have any cases
closed without arrest and/or open/no arrest. We used the `pivot wider`
function to create a table that’s easier to read with the columns being
the disposition status. We created a new variable called
`total_unsolved_homicides` that summed the dispositions giving us the
total unsolved homicides per city in the US. We called this new dataset
`unsolved_homicides`.

``` r
unsolved_homicides =
  homicide_df %>% 
      group_by(city_state, disposition) %>% 
       filter(disposition %in% c('Closed without arrest', 'Open/No arrest')) %>% 
        summarize(n = n()) %>% 
     
    mutate(
      city_state = as.factor(city_state)) %>% 
        arrange(desc(n)) %>% 
          pivot_wider(
            names_from = disposition,
              values_from = n
  ) %>% 
      janitor::clean_names() %>% 
           replace(is.na(.),0) %>% 
  mutate(total_unsolved_homicides = sum(open_no_arrest, closed_without_arrest))
```

    ## `summarise()` has grouped output by 'city_state'. You can override using the
    ## `.groups` argument.

From our analysis, Chicago has the largest number of unsolved homicides
in the US (4,073) with 3,686 cases still open without any arrest and 387
cases closed without arrest. On the other hand, Tampa has the least
number of unsolved homicides in the US (95) with 87 cases still open
without arrest and 8 cases closed without arrest.

We now merge the two datasets created above, `tot_homicides` and
`unsolved_homicides`. with 50 US cities (Tulsa, AL was ommitted from
newly created dataset) and nested our homicide totals for each city.
This will allow us to perform additional manipulations and answer
specific questions using the variables created from the analysis
conducted to create the datasets. We kept the following variables in the
newly created dataset, `homicide_mrg` : `city_state`,
`total_unsolved_homicides`, `tot_number_homicides`.

``` r
homicide_mrg =
  merge(unsolved_homicides, tot_homicides) %>% 
    select(city_state, total_unsolved_homicides, tot_number_homicides) %>% 
      arrange(desc(tot_number_homicides)) 
```

Now we examine the proportion of homicides that are unsolved for the
city of Baltimore by using the `prop.test` function.`homicide_prop`was
created as a list that contains the proportion estimates and confidence
intervals for each city. We utilize the function created above to run
this procedure making use of `purrr::map` and `unnest` to produce a tidy
dataframe. Output was saved as `prop_baltimore`.

``` r
prop_baltimore =
  homicide_mrg %>% 
  filter(city_state %in% c('Baltimore,MD')) %>% 
  mutate( homicide_prop =
            map2(.x=total_unsolved_homicides, .y=tot_number_homicides, .f=prop_fx)) %>% 
  unnest(homicide_prop) %>% 
  select(estimate, conf.low, conf.high) 
 

prop_baltimore %>% 
   rename('Lower level CI'= conf.low ,
         'Upper Level CI' = conf.high,
         Proportion = estimate)
```

    ## # A tibble: 1 × 3
    ##   Proportion `Lower level CI` `Upper Level CI`
    ##        <dbl>            <dbl>            <dbl>
    ## 1       64.6             62.8             66.3

64.56 of homicides with a confidence interval of 62.76 and 66.32 are
unsolved in the city of Baltimore, MD.

Now we run `prop.test` for each of the cities in our dataset, and
extract both the proportion of unsolved homicides and the confidence
interval for each city. We apply the function created above making use
of `purrr::map` and `unnest` to produce a tidy dataframe.`homicide_prop`
variable was created as a list that contains the proportion estimates
and confidence intervals for each city. Output was saved as
`prop_cities`.

``` r
prop_cities=
  homicide_mrg %>% 
      mutate(homicide_prop =
               map2(.x =total_unsolved_homicides,.y=tot_number_homicides,
                    .f= ~prop_fx(x = .x, n = .y))) %>% 
    unnest(homicide_prop) %>% 
  select(city_state,estimate, conf.low, conf.high)

prop_cities %>% 
    rename('Lower level CI'= conf.low ,
         'Upper Level CI' = conf.high,
         Proportion = estimate,
         City = city_state)
```

    ## # A tibble: 50 × 4
    ##    City            Proportion `Lower level CI` `Upper Level CI`
    ##    <fct>                <dbl>            <dbl>            <dbl>
    ##  1 Chicago,IL            73.6             72.4             74.7
    ##  2 Philadelphia,PA       44.8             43               46.6
    ##  3 Houston,TX            50.8             48.9             52.6
    ##  4 Baltimore,MD          64.6             62.8             66.3
    ##  5 Detroit,MI            58.8             56.9             60.8
    ##  6 Los Angeles,CA        49               46.9             51.1
    ##  7 St. Louis,MO          54.0             51.5             56.4
    ##  8 Dallas,TX             48.1             45.6             50.6
    ##  9 Memphis,TN            31.9             29.6             34.3
    ## 10 New Orleans,LA        64.8             62.3             67.3
    ## # … with 40 more rows

Let’s create a plot that shows the estimates and confidence intervals
for each city adding error bars based on the upper and lower limits of
the confidence interval. Cities are organized according to the
proportion of unsolved homicides

``` r
prop_cities %>% 
  
  ggplot(aes(x= fct_reorder(city_state, -estimate), y= estimate)) + 
  geom_bar(stat="identity", color="black", fill="lightblue",
           position=position_dodge()) +
  scale_y_continuous(breaks = c(0,20,40,60,80,100)) +
  
  geom_errorbar(aes(ymin =conf.low, ymax= conf.high), width= .2,
                position=position_dodge(.9)) + 
  
  labs(title = "Proportion of unsolved homicides across 50 US cities") +
  xlab("Cities") +
  ylab("Proportions") +
  theme(axis.text.x = element_text(angle = 60, hjust = 1))
```

<img src="p8105_hw5_mbc2178_files/figure-gfm/unnamed-chunk-11-1.png" width="90%" />

### Problem 3

In this problem, we conduct a simulation to explore power in a
one-sample t-test. We defined a function above with these fixed
criteria, sample size(n) = 30 and sigma (sd) = 5.

First, we set the mean = 0 and generate 5,000 datasets following a
normal distribution to create a tidy dataframe by making use of
`expand_grid`, `map2` and `unnest` functions. We performed a t-test
setting our null hypothesis = 0 with a 95% confidence interval. We used
the `broom::tidy` function to clean the output of the t-test and saved
the estimated mean and corresponding p-value.

Performed a t-test for a true mean = 0.

``` r
sim_results_zero_df =
  expand_grid(
    true_mu = 0,
    iteration = 1:100
   ) %>% 
  mutate(
    estimate_df = 
      map(.x = true_mu, ~ttest_fx(true_mu = .x))
  ) %>% 
  unnest(estimate_df) 
```

We repeated a similar procedure for true mean = 1,2,3,4,5,and 6.

``` r
sim_multiple_df =
  expand_grid(
    true_mu = 1:6,
    iteration = 1:100
   ) %>% 
  mutate(
    estimate_df= 
      map(.x = true_mu, ~ttest_fx(true_mu = .x))
  ) %>% 
  unnest(estimate_df) 
```
